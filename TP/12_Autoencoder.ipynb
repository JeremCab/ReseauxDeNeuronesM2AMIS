{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aeaea7b",
   "metadata": {
    "id": "0aeaea7b"
   },
   "source": [
    "# Autoencoders (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5dda87",
   "metadata": {},
   "source": [
    "The **MNIST dataset** consists of 60,000 images of hand written digit, where each image has size 28X28.\n",
    "\n",
    "We will define and train two **autoencoders (AEs)** on the MNIST dataset:\n",
    "- A simple autoencoder composed of linear layers\n",
    "- A more complex autoencoder composed of convolutional layers.\n",
    "\n",
    "<img src=\"files/figures/mnist.png\" width=\"600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef1b769",
   "metadata": {},
   "source": [
    "## 0. Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc268c45",
   "metadata": {
    "id": "1c9112e6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bde5cd",
   "metadata": {
    "id": "81bde5cd"
   },
   "source": [
    "## 1. Load MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54312009",
   "metadata": {},
   "source": [
    "- The following code loads the MNIST data.\n",
    "- Note that we don't need any validation set, since we are in an unsupervised setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477472c8",
   "metadata": {
    "id": "477472c8"
   },
   "outputs": [],
   "source": [
    "# data\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "mnist_data = datasets.MNIST(root='./data', train=True,\n",
    "                            download=True,\n",
    "                            transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a90cf3",
   "metadata": {
    "id": "91a90cf3"
   },
   "outputs": [],
   "source": [
    "# dataloader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader) * 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ee540b",
   "metadata": {
    "id": "f3ee540b"
   },
   "outputs": [],
   "source": [
    "# data format\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52e4a4a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e52e4a4a",
    "outputId": "7cea6510-3a70-40c3-ede6-6e930f416e0d"
   },
   "outputs": [],
   "source": [
    "torch.min(images), torch.max(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2783f7a8",
   "metadata": {
    "id": "2783f7a8"
   },
   "source": [
    "## 2. AE with linear layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df53c611",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1461305d",
   "metadata": {},
   "source": [
    "The following class implements an **autoencoder (AE)** composed of **linear layers**.\n",
    "\n",
    "- Understand the architecture of this **autonecoder (AE)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738248da",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e193e89b",
   "metadata": {
    "id": "e193e89b"
   },
   "outputs": [],
   "source": [
    "class Autoencoder_Linear(nn.Module):\n",
    "    \"\"\"Implements an linear automencoder\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"constructor\"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 2) # -> N, 2 only!\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 28 * 28),\n",
    "            nn.Sigmoid()             # output neurons between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"forward pass\"\"\"\n",
    "        \n",
    "        encoded_data = self.encoder(x)\n",
    "        decoded_data = self.decoder(encoded_data)\n",
    "\n",
    "        return decoded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0141f45",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2e8d3d",
   "metadata": {},
   "source": [
    "- Define a **loss (`MSEloss`)** and an **optimizer (`torch.optim.Adam`)** with learning rate `lr=1e-3` for this model.\n",
    "\n",
    "- Implement a **training loop** for this model during $24$ epochs. During training, after each epoch, store the current `epoch`, `inputs`, `outputs` and `train_loss` of the model (in this order) in a list called `outputs_l`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330aa32e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c62d2b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23def0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "LR8JnW54EVnc",
   "metadata": {
    "id": "LR8JnW54EVnc"
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00ec8ea",
   "metadata": {},
   "source": [
    "- Plot the training loss to confirm that the model has been trained.\n",
    "\n",
    "- Run the following code which displays the inputs and their reconstructions by the autoencoder at epochs $1$, $12$ and $24$. The reconstructions should improve as the epochs increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5d351f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3afca31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cf7860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a73c2c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 724
    },
    "id": "3a73c2c7",
    "outputId": "9b2280b6-0c12-4801-ffed-a44e214ab5ee"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(6, 10, \n",
    "                        figsize=(10, 7), \n",
    "                        layout=\"constrained\",\n",
    "                        gridspec_kw={'wspace': 0.1, 'hspace': 0.1})\n",
    "\n",
    "for i, k in enumerate([0, num_epochs//2, num_epochs-1]):\n",
    "\n",
    "    inputs = outputs_l[k][1].detach().numpy()\n",
    "    outputs = outputs_l[k][2].detach().numpy()\n",
    "\n",
    "    axs[2*i, 0].set_title(f\"Epoch {k+1}\")\n",
    "    axs[2*i, 0].set_ylabel(\"Inputs\")\n",
    "\n",
    "    for j, item in enumerate(inputs):\n",
    "\n",
    "        if j >= 10: break\n",
    "        item = item.reshape(-1, 28,28) # for Autoencoder_Linear\n",
    "        axs[2*i, j].imshow(item[0])\n",
    "        axs[2*i, j].set_xticks([])\n",
    "        axs[2*i, j].set_yticks([])\n",
    "\n",
    "    axs[2*i + 1, 0].set_ylabel(\"Outputs\")\n",
    "    for j, item in enumerate(outputs):\n",
    "\n",
    "        if j >= 10: break\n",
    "        item = item.reshape(-1, 28,28) # for Autoencoder_Linear\n",
    "        axs[2*i + 1, j].imshow(item[0])\n",
    "        axs[2*i + 1, j].set_xticks([])\n",
    "        axs[2*i + 1, j].set_yticks([])\n",
    "\n",
    "fig.savefig('figures/AE_linear.pdf', format='pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d0d7f0",
   "metadata": {
    "id": "b8d0d7f0"
   },
   "source": [
    "## 3. AE with convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61285cd",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b32419",
   "metadata": {},
   "source": [
    "The following class implements an **autoencoder (AE)** composed of **convolutional layers**.\n",
    "\n",
    "- Understand the architecture of this **autonecoder (AE)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf5eae2",
   "metadata": {
    "id": "bdf5eae2"
   },
   "outputs": [],
   "source": [
    "class Autoencoder_CNN(nn.Module):\n",
    "    \"\"\"Implements a CNN autoencoder\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"constructor\"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # N, 1, 28, 28\n",
    "        self.encoder = nn.Sequential(\n",
    "            # -> N, 16, 14, 14\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # -> N, 32, 7, 7\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # -> N, 64, 1, 1\n",
    "            nn.Conv2d(32, 64, 7)\n",
    "            )\n",
    "\n",
    "        # N , 64, 1, 1\n",
    "        self.decoder = nn.Sequential(\n",
    "            # -> N, 32, 7, 7\n",
    "            nn.ConvTranspose2d(64, 32, 7),\n",
    "            nn.ReLU(),\n",
    "            # N, 16, 14, 14 (N, 16, 13, 13 without output_padding)\n",
    "            nn.ConvTranspose2d(32, 16, 3, \n",
    "                               stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            # N, 1, 28, 28 (N,1, 27, 27)\n",
    "            nn.ConvTranspose2d(16, 1, 3, \n",
    "                               stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"forward function\"\"\"\n",
    "\n",
    "        encoded_data = self.encoder(x)\n",
    "        decoded_data = self.decoder(encoded_data)\n",
    "\n",
    "        return decoded_data\n",
    "\n",
    "\n",
    "# Note: nn.MaxPool2d -> use nn.MaxUnpool2d, or use different kernelsize, stride etc to compensate...\n",
    "# Input [-1, +1] -> use nn.Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TZ8DlMzvEQyG",
   "metadata": {
    "id": "TZ8DlMzvEQyG"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104e6b4d",
   "metadata": {},
   "source": [
    "- Define a **loss** (`MSEloss`) and an **optimizer** (`torch.optim.Adam`) with learning rate `lr=1e-3` for this model.\n",
    "\n",
    "- Implement a **training loop** for this model during $12$ epochs. During training, after each epoch, store the current `epoch`, `inputs`, `outputs` and `train_loss` of the model (in this order) in a list called `outputs_l`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b694465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926d92d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec621724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "_2fUW-QIEY_T",
   "metadata": {
    "id": "_2fUW-QIEY_T"
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a105a700",
   "metadata": {},
   "source": [
    "- Plot the training loss to confirm that the model has been trained.\n",
    "\n",
    "- Run the following code which displays the inputs and their reconstructions by the autoencoder at epochs $1$, $7$ and $12$. The reconstructions should improve as the epochs increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c984f8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9097e20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0392badc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104TjvdYEZi1",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "104TjvdYEZi1",
    "outputId": "d0b40705-39e3-48c7-dfd7-4fb1e6781782"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(6, 10, \n",
    "                        figsize=(10, 7), \n",
    "                        layout=\"constrained\",\n",
    "                        gridspec_kw={'wspace': 0.1, 'hspace': 0.1})\n",
    "\n",
    "for i, k in enumerate([0, num_epochs//2, num_epochs-1]):\n",
    "\n",
    "    inputs = outputs_l[k][1].detach().numpy()\n",
    "    outputs = outputs_l[k][2].detach().numpy()\n",
    "\n",
    "    axs[2*i, 0].set_title(f\"Epoch {k+1}\")\n",
    "    axs[2*i, 0].set_ylabel(\"Inputs\")\n",
    "\n",
    "    for j, item in enumerate(inputs):\n",
    "\n",
    "        if j >= 10: break\n",
    "        axs[2*i, j].imshow(item[0])\n",
    "        axs[2*i, j].set_xticks([])\n",
    "        axs[2*i, j].set_yticks([])\n",
    "\n",
    "    axs[2*i + 1, 0].set_ylabel(\"Outputs\")\n",
    "    for j, item in enumerate(outputs):\n",
    "\n",
    "        if j >= 10: break\n",
    "        axs[2*i + 1, j].imshow(item[0])\n",
    "        axs[2*i + 1, j].set_xticks([])\n",
    "        axs[2*i + 1, j].set_yticks([])\n",
    "\n",
    "fig.savefig('figures/AE_cnn.pdf', format='pdf', bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08e9cbe",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "1OWoydIwR1fa"
   },
   "source": [
    "## 4. Visualizing the latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1f460d",
   "metadata": {},
   "source": [
    "- Select about 1000 input images, about 100 samples of each input digit (0, 1, 2, ..., 9) (around 1000 / 64 = 16 batches), from your dataset.\n",
    "\n",
    "- Compute the **encodings** of these samples in the **2D latent space**. For this purpose, note that the **encoder** part of the **autoencoder** `model` can be selected using tthe following instruction: `encoder = model.encoder.eval()`\n",
    "\n",
    "- Plot the encoded data in the 2D latent space with their original labels in different colors. This will give you an idea of how the different data are distributed in the latent sapce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f39a3d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c627b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ed60f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99888427",
   "metadata": {},
   "source": [
    "## 5. Generative capabilities of the decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d4e56",
   "metadata": {},
   "source": [
    "### 5.1: Uniform sampling from the latent space\n",
    "\n",
    "- By considering the 1000 encoded data of the preovous exercice, estimate the minimal and maximal coordinates $x$ and $y$ of your encoded data in the 2D-latent space ($x_{min}$, $x_{max}$, $y_{min}$, $y_{max}$).\n",
    "\n",
    "- Sample 100 2D-points from the latent space according to the uniform distribution $\\mathcal{U}([x_{min}, x_{max}], [y_{min}, y_{max}])$.<br>\n",
    "For this purpose, you can use the instruction:<br> \n",
    "`np.random.uniform(low=xy_min, high=xy_max, size=(100,2))`\n",
    "\n",
    "- Compute the **decoded images** of these 2D-points. For this purpose, note that the **decoder** part of the **autoencoder** `model` can be selected using tthe following instruction: `decoder = model.decoder.eval()`. Store these images in a list called `decoded_samples`.\n",
    "\n",
    "- At this stage, you have **generated** new images by *sampling* the 2D-latent space and *decoding* these samples. Plot these decoded images. Are these images of good quality? Do they resemble to the digits of the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6495c4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3544e7fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac64775e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5678ea48",
   "metadata": {},
   "source": [
    "### 5.2 Normal sampling from the latent space\n",
    "\n",
    "- By considering the 1000 encoded data of the preovous exercice, estimate the mean $\\mu$ and variance $\\sigma^2$ of your encoded data in the 2D-latent space.\n",
    "\n",
    "- Sample 100 2D-points from the latent space according to the normal distribution $\\mathcal{UN}(\\mu, \\sigma^2)$.<br>\n",
    "For this purpose, you can use the instruction:<br> \n",
    "`np.random.multivariate_normal(mean, cov, (100))`<br>\n",
    "where `cov` is the diagonal matrix where the diagonal is the variance vector.\n",
    "\n",
    "- Compute the **decoded images** of these 2D-points. For this purpose, note that the **decoder** part of the **autoencoder** `model` can be selected using tthe following instruction: `decoder = model.decoder.eval()`. Store these images in a list called `decoded_samples`.\n",
    "\n",
    "- At this stage, you have **generated** new images by *sampling* the 2D-latent space and *decoding* these samples. Plot these decoded images. Are these images of good quality? Do they resemble to the digits of the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9ceda9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc9a6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b6b435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "140df21a",
   "metadata": {},
   "source": [
    "**Conclusion:** In both cases, the generated data are somewhat blurry and some numbers are over-represented."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
