{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "326d77b0",
   "metadata": {},
   "source": [
    "#Â Backpropagation (BP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4941630",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03b340ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50e5997",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569c8082",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "The **backpropagation algorithm (BP)** is an efficient implementation of (mini-batch) stochastic gradient descent for neural networks.\n",
    "    \n",
    "In short, the algorithm consists of:\n",
    "1. a **forward pass**: computation of the activations;\n",
    "2. a **backward pass**: computation of the gradients;\n",
    "3. an **updating step**: updating of the weights and biases using gradients.\n",
    "\n",
    "<img src=\"figures/backprop.png\" width=\"800px\"/>\n",
    "    \n",
    "We will use the following **sigmoid activation function**\n",
    "    $$\n",
    "    \\sigma(z) = \\frac{1}{1 + \\exp(-z)}\n",
    "    $$\n",
    "whose derivative is given by\n",
    "    $$\n",
    "    \\sigma'(z) = \\sigma(z) \\cdot \\left( 1 - \\sigma(z) \\right).\n",
    "    $$\n",
    "\n",
    "Furthermore, we will use the following **quadratic loss function**\n",
    "    $$\n",
    "    \\mathcal{L_k} \\left( \\Theta \\right) := \\mathcal{L_k} \\left( \\Theta, \\boldsymbol{a_k^{[L]}}, \\boldsymbol{y_k} \\right) = \\frac{1}{2} \\left\\| \\boldsymbol{a_k^{[L]}} - \\boldsymbol{y_k} \\right\\|^{2}\n",
    "    $$\n",
    "whose gradient with respect to $\\boldsymbol{a_k^{[L]}}$ is given by\n",
    "    $$\n",
    "    \\nabla_{\\boldsymbol{a_k^{[L]}}} \\mathcal{L_k} \\left( \\Theta \\right) = \\boldsymbol{a_k^{[L]}} - \\boldsymbol{y_k}.\n",
    "    $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48445972",
   "metadata": {},
   "source": [
    "**Step 1**\n",
    "- Implement the activation functions ``sigma(z)`` and its derivative ``sigma_prime(z)``.\n",
    "- Implement the loss function ``loss(a_k, y_k)`` and gradient ``loss_gradient(a_k, y_k)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62488007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b9acec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42fadad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03370880",
   "metadata": {},
   "source": [
    "**Step 2**\n",
    "- Create a class `Network()` which takes a list `[n0, n2, ..., nL]` as parameter and creates an MLP with $L+1$ layers of $n_i$ neurons each, for $i= 0, \\dots, L$.\n",
    "- Initializes the weights matrices $\\boldsymbol{W^{[l]}}$ and the bias vectors $\\boldsymbol{b^{[l]}}$ randomly from a normal distribution $\\mathcal{N}(0, 1)$ (`torch.normal()`), for $l = 0, \\dots, L-1$.<br>\n",
    "(Note that according to our notation, for $L+1$ layers, there are $L$ weights and $L$ biases.)\n",
    "- The first layer is the input layer and thus has no biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abe1490",
   "metadata": {},
   "source": [
    "**Step 3**\n",
    "- Implement a method ``forward_pass(self, X)`` which:\n",
    "    - takes as inputs a batch of data $\\boldsymbol{X}$ (2D tensor)\n",
    "    - returns as outputs:\n",
    "        - the list of pre-activations ``Z_l`` = $\\left[ \\boldsymbol{Z}^{[0]}, \\boldsymbol{Z}^{[1]}, \\dots, \\boldsymbol{Z}^{[L]} \\right]$ associated to $\\boldsymbol{X}$ (list of 2D tensors).\n",
    "        - the list of activations ``A_l`` = $\\left[ \\boldsymbol{A}^{[0]}, \\boldsymbol{A}^{[1]}, \\dots, \\boldsymbol{A}^{[L]} \\right]$ associated to $\\boldsymbol{X}$ (list of 2D tensors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e518fb2",
   "metadata": {},
   "source": [
    "**Step 4**\n",
    "- Implement a method ``backward_pass(self, Z_l, A_l, Y, eta)`` which:\n",
    "    - takes as inputs:\n",
    "        - the list of pre-activations ``Z_l`` = $\\left[ \\boldsymbol{Z}^{[0]}, \\boldsymbol{Z}^{[1]}, \\dots, \\boldsymbol{Z}^{[L]} \\right]$ (list of 2D tensors).\n",
    "        - the list of activations ``A_l`` = $\\left[ \\boldsymbol{A}^{[0]}, \\boldsymbol{A}^{[1]}, \\dots, \\boldsymbol{A}^{[L]} \\right]$ (list of 2D tensors).\n",
    "        - the batch of targets ``Y`` (2D tensor);\n",
    "        - the learning rate ``eta`` (float).\n",
    "    - updates the attributes ``self.weights`` and ``self.biases`` according to the backward pass.\n",
    "    \n",
    "**Remark:** note that ``Z_l`` and ``A_l`` contain one more element that ``self.weights``. Accordingly, indices might get confusing. The following picture clarifies the situation:<br>\n",
    "```\n",
    "In algo: z^[0]                z^[1]                z^[2]                 ...\n",
    "In code: Z_l[0]               Z_l[1]               Z_l[2]                ...\n",
    "In algo: a^[0]                a^[1]                a^[2]                 ...\n",
    "In code: A_l[0]               A_l[1]               A_l[2]                ...\n",
    "In algo:            W^[1]                W^[2]                W^[3]      ...\n",
    "In code:       self.weights[0]      self.weights[1]      self.weights[2] ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919058d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187b7639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b217a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da4f576d",
   "metadata": {},
   "source": [
    "**Step 5**\n",
    "- Using your class ``Network``, initialize a network as follows:<br>\n",
    "    ``net = Network([100, 200, 300, 150, 5])``\n",
    "- Implement a forward pass on a random tensor ``X`` of size $100 \\times 32$.\n",
    "- Implement a backward pass using a random tensor ``Y`` of size $5 \\times 32$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37201e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b5f8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9c0aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1edac4cd",
   "metadata": {},
   "source": [
    "# Application to the MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f293f4a4",
   "metadata": {},
   "source": [
    "The **MNIST dataset** consists of handwritten digits. The MNIST classification problem consists in predicting the correct digit represented on an image.\n",
    "\n",
    "<img src=\"files/figures/mnist.png\" width=\"600px\"/>\n",
    "\n",
    "- Load the train and test MNIST datasets using the following commands:\n",
    "```\n",
    "train = datasets.MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "test = datasets.MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
    "```\n",
    "Each sample consists of a tensor (the image encoded in black and white), and a label (the digit that it represents).\n",
    "- Examine the train and test sets.\n",
    "- Visualize some data samples (tensors) using `plt.imshow()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5d0765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4705307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9af67f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1b5b9e2",
   "metadata": {},
   "source": [
    "A **dataloader** creates batches of samples from a dataset so that they can be passed into a model.\n",
    "- Create a train and test dataloaders using the following commands:\n",
    "```\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=True)\n",
    "```\n",
    "- Note that dataloaders are not subscriptable.\n",
    "- Try to catch one batch of the dataloader and examine it.\n",
    "- Write a function that reshapes a batch of size $32 \\times 1 \\times 28 \\times 28$ into a tensor of size $784 \\times 32$.<br>\n",
    "(use `torch.squeeze()`, `torch.reshape()`, `torch.flatten()`, `torch.transpose()`, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5893b097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8ca426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c61b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4bbb26e",
   "metadata": {},
   "source": [
    "Instantiate a 4-layer MLP with the following characteristics:\n",
    "- Layer 1 (or input layer): size 784\n",
    "- Layer 2: size 128\n",
    "- Layer 3: size 128\n",
    "- Layer 4 (or output layer): size 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee8cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a28a679d",
   "metadata": {},
   "source": [
    "- Train your network for 40 epochs with a learning rate of $0.1$.\n",
    "- Write a ``train()``function that uses your methods ``forward_pass()`` and ``backward_pass()``.\n",
    "- Compute and store the loss after each batch processing:<br>\n",
    "    You can one-hot encode the targets and use your function ``loss()``.<br>\n",
    "    https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html\n",
    "- Print the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d345dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae6c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81fcd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bf657e4",
   "metadata": {},
   "source": [
    "- Write a function ``predict(network, dataloader)`` that returns the targets and predictions of a dataset.\n",
    "- Compute the classification reports for the train and test sets:<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6109b5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a269b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e071e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
